import streamlit as st 
import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import CategoricalNB, GaussianNB
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, precision_score, recall_score, f1_score
from sklearn.impute import SimpleImputer
import xgboost as xgb
import warnings
warnings.filterwarnings('ignore')

#################### Page configuration #########
st.set_page_config(layout="wide", page_title="TP3 - Classification Supervis√©e")

# Header avec style acad√©mique
st.markdown("""
<div style='text-align: center; padding: 20px; background-color: #f0f2f6; border-radius: 10px; margin-bottom: 30px;'>
    <h1> Machine Learning I</h1>
    <h2>TP3 - Classification Supervis√©e</h2>
    <p><em>Lecture 3: Supervised Learning - Distance-based approaches, Decision trees, Naive Bayes Classifiers</em></p>
    <p><strong><b>Realis√© par:<b> Emmanuel M. Morris <strong><p>
    <p><strong><b>Encadr√© par:<b> Issam Falih - Department of Computer Science</strong></p>
</div>
""", unsafe_allow_html=True)

# Sidebar pour navigation
st.sidebar.title("Navigation")
st.sidebar.markdown("### Choisir la section d'analyse")
section = st.sidebar.selectbox(
    "Dataset √† analyser",
    ["Partie I: Titanic Dataset", "Partie II: Heart Disease Dataset"]
)

# Rappel th√©orique dans la sidebar
st.sidebar.markdown("---")
st.sidebar.markdown("### Rappels Th√©oriques")
st.sidebar.markdown("""
**Supervised Learning Workflow:**
1. Training set (labels connus)
2. Validation set (√©valuation)
3. Test set (pr√©diction)

**Classifiers √©tudi√©s:**
- **Naive Bayes**: Probabiliste bas√© sur le th√©or√®me de Bayes
- **Decision Trees**: R√®gles de d√©cision hi√©rarchiques
- **XGBoost**: Ensemble method (boosting)

**M√©triques d'√©valuation:**
- Accuracy = (TP + TN) / Total
- Precision = TP / (TP + FP)
- Recall = TP / (TP + FN)
- F-Measure = 2√ó(Precision√óRecall)/(Precision+Recall)
""")

# Fonctions utilitaires
@st.cache_data
def load_data():
    """Charge tous les datasets selon le workflow supervis√©"""
    try:
        titanic_train = pd.read_csv("titanic_train.csv")
        titanic_test = pd.read_csv("titanic_test.csv")
        heart_data = pd.read_csv("heart-disease-UCI.csv")
        return titanic_train, titanic_test, heart_data
    except FileNotFoundError as e:
        st.error(f"Erreur de chargement des donn√©es: {e}")
        st.info("Assurez-vous que les fichiers CSV sont dans le bon r√©pertoire")
        return None, None, None

def plot_confusion_matrix_with_metrics(y_true, y_pred, title):
    """Affiche la matrice de confusion avec m√©triques d√©taill√©es"""
    cm = confusion_matrix(y_true, y_pred)
    
    # Calcul des m√©triques selon les notes de cours
    TP = cm[1, 1] if cm.shape == (2, 2) else None
    TN = cm[0, 0] if cm.shape == (2, 2) else None
    FP = cm[0, 1] if cm.shape == (2, 2) else None
    FN = cm[1, 0] if cm.shape == (2, 2) else None
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    # Matrice de confusion
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax1)
    ax1.set_title(f'Matrice de Confusion - {title}')
    ax1.set_xlabel('Pr√©dictions')
    ax1.set_ylabel('Valeurs R√©elles')
    
    # M√©triques d√©taill√©es
    if TP is not None:
        accuracy = (TP + TN) / (TP + TN + FP + FN)
        precision = TP / (TP + FP) if (TP + FP) > 0 else 0
        recall = TP / (TP + FN) if (TP + FN) > 0 else 0
        specificity = TN / (TN + FP) if (TN + FP) > 0 else 0
        f_measure = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
        
        metrics_text = f"""
        M√©triques de Classification (selon cours):
        
        TP (True Positive): {TP}
        TN (True Negative): {TN}
        FP (False Positive): {FP}
        FN (False Negative): {FN}
        
        Accuracy = (TP + TN) / Total = {accuracy:.4f}
        Precision = TP / (TP + FP) = {precision:.4f}
        Recall (TPR) = TP / (TP + FN) = {recall:.4f}
        Specificity = TN / (TN + FP) = {specificity:.4f}
        F-Measure = 2√ó(P√óR)/(P+R) = {f_measure:.4f}
        """
        
        ax2.text(0.1, 0.5, metrics_text, fontsize=10, verticalalignment='center',
                bbox=dict(boxstyle="round,pad=0.3", facecolor="lightblue"))
        ax2.set_xlim(0, 1)
        ax2.set_ylim(0, 1)
        ax2.axis('off')
        ax2.set_title('M√©triques d\'√âvaluation')
    
    plt.tight_layout()
    return fig

def preprocess_titanic_data(df):
    """Pr√©processing selon les 5 √©tapes du supervised learning"""
    df_processed = df.copy()
    
    # √âtape 2: D√©terminer les features d'entr√©e et leurs repr√©sentations
    st.write("**√âtape 2: D√©termination des features d'entr√©e**")
    
    # Gestion des valeurs manquantes
    df_processed['Age'].fillna(df_processed['Age'].median(), inplace=True)
    df_processed['Fare'].fillna(df_processed['Fare'].median(), inplace=True)
    df_processed['Embarked'].fillna(df_processed['Embarked'].mode()[0], inplace=True)
    
    # Encodage des variables cat√©gorielles pour les classifiers
    le_sex = LabelEncoder()
    le_embarked = LabelEncoder()
    
    df_processed['Sex_encoded'] = le_sex.fit_transform(df_processed['Sex'])
    df_processed['Embarked_encoded'] = le_embarked.fit_transform(df_processed['Embarked'])
    
    # Cr√©ation de nouvelles features (feature engineering)
    df_processed['Child'] = (df_processed['Age'] < 18).astype(int)
    df_processed['FamilySize'] = df_processed['SibSp'] + df_processed['Parch'] + 1
    
    return df_processed, le_sex, le_embarked

# Chargement des donn√©es
titanic_train, titanic_test, heart_data = load_data()

if titanic_train is None:
    st.stop()

###################### PARTIE I: TITANIC DATASET #################
if section == "Partie I: Titanic Dataset":
    st.markdown("# Partie I: Titanic Dataset")
    st.markdown("### *Application des concepts de classification supervis√©e*")
    st.markdown("---")
    
    # √âtape 1: D√©cider du training set repr√©sentatif
    st.markdown("## √âtape 1: Analyse du Training Set")
    st.markdown("*D√©cider d'un training set repr√©sentatif du monde r√©el*")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("#### Aper√ßu des donn√©es d'entra√Ænement")
        st.dataframe(titanic_train.head())
        
        st.markdown("#### Informations sur le dataset")
        info_df = pd.DataFrame({
            "Attributs": titanic_train.columns,
            "Type": titanic_train.dtypes.astype(str),
            "Non-null": titanic_train.notnull().sum(),
            "Valeurs manquantes": titanic_train.isnull().sum(),
            "% Manquant": round(titanic_train.isnull().mean()*100, 2)
        })
        st.dataframe(info_df)
    
    with col2:
        st.markdown("#### Statistiques descriptives")
        st.dataframe(titanic_train.describe())
        
        # Distribution de la variable cible
        st.markdown("#### Distribution de la classe cible")
        survival_counts = titanic_train['Survived'].value_counts()
        fig, ax = plt.subplots(figsize=(8, 6))
        ax.pie(survival_counts.values, labels=['D√©c√©d√© (0)', 'Survivant (1)'], 
               autopct='%1.1f%%', colors=['lightcoral', 'lightblue'])
        ax.set_title('Distribution de la Variable Cible (Survived)')
        st.pyplot(fig)
    
    # Analyse exploratoire selon les concepts du cours
    st.markdown("##  Analyse Exploratoire des Donn√©es")
    
    with st.expander("Visualisations des distributions"):
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # Distribution de l'√¢ge
        sns.histplot(titanic_train['Age'].dropna(), kde=True, ax=axes[0,0], color="skyblue")
        axes[0,0].set_title("Distribution de l'√Çge")
        axes[0,0].set_xlabel("√Çge")
        
        # Distribution par classe
        sns.countplot(x='Pclass', data=titanic_train, palette="viridis", ax=axes[0,1])
        axes[0,1].set_title("R√©partition par Classe")
        
        # Survie par sexe
        survival_by_sex = titanic_train.groupby('Sex')['Survived'].mean()
        axes[1,0].bar(survival_by_sex.index, survival_by_sex.values, color=['pink', 'lightblue'])
        axes[1,0].set_title("Taux de Survie par Sexe")
        axes[1,0].set_ylabel("Probabilit√© de Survie")
        
        # Survie par classe
        survival_by_class = titanic_train.groupby('Pclass')['Survived'].mean()
        axes[1,1].bar(survival_by_class.index, survival_by_class.values, color='lightgreen')
        axes[1,1].set_title("Taux de Survie par Classe")
        axes[1,1].set_ylabel("Probabilit√© de Survie")
        
        plt.tight_layout()
        st.pyplot(fig)
    
    # Analyse "Les femmes et les enfants d'abord" avec approche bay√©sienne
    st.markdown("## Analyse Bay√©sienne: 'Les femmes et les enfants d'abord'")
    st.markdown("*Application du th√©or√®me de Bayes pour analyser les facteurs de survie*")
    
    # Cr√©ation des groupes d'√¢ge
    titanic_train["Age_Category"] = titanic_train["Age"].apply(
        lambda x: "Enfant" if pd.notna(x) and x < 18 else "Adulte"
    )
    
    # Calcul des probabilit√©s selon Bayes
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("#### P(Survie | Sexe)")
        survival_by_sex = titanic_train.groupby("Sex")["Survived"].agg(['mean', 'count']).round(4)
        survival_by_sex.columns = ['P(Survie|Sexe)', 'Effectif']
        st.dataframe(survival_by_sex)
        
        # Visualisation
        fig, ax = plt.subplots()
        survival_by_sex['P(Survie|Sexe)'].plot(kind='bar', ax=ax, color=['pink', 'lightblue'])
        ax.set_title('P(Survie | Sexe)')
        ax.set_ylabel('Probabilit√©')
        ax.set_xticklabels(['Femme', 'Homme'], rotation=0)
        st.pyplot(fig)
    
    with col2:
        st.markdown("#### P(Survie | √Çge)")
        survival_by_age = titanic_train.groupby("Age_Category")["Survived"].agg(['mean', 'count']).round(4)
        survival_by_age.columns = ['P(Survie|√Çge)', 'Effectif']
        st.dataframe(survival_by_age)
        
        # Visualisation
        fig, ax = plt.subplots()
        survival_by_age['P(Survie|√Çge)'].plot(kind='bar', ax=ax, color=['orange', 'green'])
        ax.set_title('P(Survie | √Çge)')
        ax.set_ylabel('Probabilit√©')
        ax.set_xticklabels(['Adulte', 'Enfant'], rotation=0)
        st.pyplot(fig)
    
    # Analyse combin√©e avec hypoth√®se d'ind√©pendance naive
    st.markdown("#### Analyse Combin√©e (Hypoth√®se d'Ind√©pendance)")
    st.markdown("*Selon l'hypoth√®se naive: P(Survie|Sexe,√Çge) ‚âà P(Survie|Sexe) √ó P(Survie|√Çge)*")
    
    survival_combined = titanic_train.groupby(["Age_Category", "Sex"])["Survived"].agg(['mean', 'count']).round(4)
    survival_combined.columns = ['P(Survie|Sexe,√Çge)', 'Effectif']
    st.dataframe(survival_combined)
    
    # Visualisation combin√©e
    fig, ax = plt.subplots(figsize=(10, 6))
    survival_pivot = titanic_train.pivot_table(values='Survived', index='Age_Category', columns='Sex', aggfunc='mean')
    sns.heatmap(survival_pivot, annot=True, cmap='RdYlBu', ax=ax, fmt='.3f')
    ax.set_title('P(Survie | Sexe, √Çge) - Matrice de Probabilit√©s')
    st.pyplot(fig)
    
    # √âtapes 3-5: Structure d'apprentissage et √©valuation
    st.markdown("##  √âtapes 3-5: Mod√®les de Classification")
    st.markdown("*Structure d'apprentissage, entra√Ænement et √©valuation*")
    
    # Pr√©paration des donn√©es (√âtape 2)
    titanic_processed, le_sex, le_embarked = preprocess_titanic_data(titanic_train)
    
    # S√©lection des features
    features = ['Pclass', 'Sex_encoded', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked_encoded', 'Child', 'FamilySize']
    X = titanic_processed[features]
    y = titanic_processed['Survived']
    
    # Division selon le workflow supervis√©
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
    
    st.success(f" **Training set**: {X_train.shape[0]} √©chantillons | **Test set**: {X_test.shape[0]} √©chantillons")
    st.info(f"**Features s√©lectionn√©es**: {len(features)} attributs")
    
    # Tabs pour les diff√©rents classifiers
    tab1, tab2, tab3, tab4 = st.tabs([
        "Naive Bayes Classifier", 
        "Decision Tree Classifier", 
        "XGBoost Classifier", 
        "Comparaison & √âvaluation"
    ])
    
    with tab1:
        st.subheader("Naive Bayes Classifier")
        st.markdown("*Bas√© sur le th√©or√®me de Bayes avec hypoth√®se d'ind√©pendance*")
        
        # Rappel th√©orique
        st.markdown("""
        **Th√©or√®me de Bayes**: 
        ```
        P(classe|x) = P(x|classe) √ó P(classe) / P(x)
        ```
        **Hypoth√®se Naive**: Les attributs sont ind√©pendants
        ```
        P(x|classe) = ‚àè P(xi|classe)
        ```
        """)
        
        if st.button("üîÑ Entra√Æner Naive Bayes", key="nb_titanic"):
            # Pour CategoricalNB, discr√©tisation des variables continues
            X_train_cat = X_train.copy()
            X_test_cat = X_test.copy()
            
            # Discr√©tisation selon les concepts du cours
            X_train_cat['Age'] = pd.cut(X_train_cat['Age'], bins=5, labels=False)
            X_test_cat['Age'] = pd.cut(X_test_cat['Age'], bins=5, labels=False)
            X_train_cat['Fare'] = pd.cut(X_train_cat['Fare'], bins=5, labels=False)
            X_test_cat['Fare'] = pd.cut(X_test_cat['Fare'], bins=5, labels=False)
            
            # Entra√Ænement
            nb_model = CategoricalNB()
            nb_model.fit(X_train_cat, y_train)
            
            # Pr√©dictions
            nb_pred = nb_model.predict(X_test_cat)
            nb_accuracy = accuracy_score(y_test, nb_pred)
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.metric("Accuracy", f"{nb_accuracy:.4f}")
                
                # Calcul des m√©triques selon le cours
                precision = precision_score(y_test, nb_pred)
                recall = recall_score(y_test, nb_pred)
                f1 = f1_score(y_test, nb_pred)
                
                st.markdown(f"""
                **M√©triques d'√©valuation:**
                - **Precision**: {precision:.4f}
                - **Recall (TPR)**: {recall:.4f}
                - **F-Measure**: {f1:.4f}
                """)
                
                st.text("Rapport de Classification:")
                st.text(classification_report(y_test, nb_pred))
            
            with col2:
                fig_cm = plot_confusion_matrix_with_metrics(y_test, nb_pred, "Naive Bayes")
                st.pyplot(fig_cm)
            
            # Analyse des probabilit√©s (concept du cours)
            st.markdown("#### Analyse des Probabilit√©s de Pr√©diction")
            nb_proba = nb_model.predict_proba(X_test_cat)
            
            fig, ax = plt.subplots(figsize=(10, 6))
            ax.hist(nb_proba[:, 1], bins=20, alpha=0.7, color='lightblue', edgecolor='black')
            ax.set_title('Distribution des P(Survie|x) - Naive Bayes')
            ax.set_xlabel('Probabilit√© de Survie')
            ax.set_ylabel('Fr√©quence')
            ax.axvline(x=0.5, color='red', linestyle='--', label='Seuil de d√©cision')
            ax.legend()
            st.pyplot(fig)
    
    with tab2:
        st.subheader("Decision Tree Classifier")
        st.markdown("*D√©composition de l'espace des features selon la variable la plus discriminante*")
        
        # Rappel th√©orique
        st.markdown("""
        **Propri√©t√©s des arbres de d√©cision:**
        - **Expressivit√©**: Peut repr√©senter des disjonctions de conjonctions
        - **Lisibilit√©**: Peut √™tre traduit en ensemble de r√®gles de d√©cision
        - **White box**: Structure facile √† comprendre et interpr√©ter
        """)
        
        if st.button("üîÑ Entra√Æner Decision Tree", key="dt_titanic"):
            # Entra√Ænement avec contr√¥le de la profondeur (bias-variance trade-off)
            dt_model = DecisionTreeClassifier(random_state=42, max_depth=5)
            dt_model.fit(X_train, y_train)
            
            # Pr√©dictions
            dt_pred = dt_model.predict(X_test)
            dt_accuracy = accuracy_score(y_test, dt_pred)
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.metric("Accuracy", f"{dt_accuracy:.4f}")
                
                # Feature importance selon l'algorithme
                feature_importance = pd.DataFrame({
                    'Feature': features,
                    'Importance': dt_model.feature_importances_
                }).sort_values('Importance', ascending=False)
                
                st.write("**Importance des Features:**")
                st.dataframe(feature_importance)
                
                # M√©triques d√©taill√©es
                precision = precision_score(y_test, dt_pred)
                recall = recall_score(y_test, dt_pred)
                f1 = f1_score(y_test, dt_pred)
                
                st.markdown(f"""
                **M√©triques d'√©valuation:**
                - **Precision**: {precision:.4f}
                - **Recall (TPR)**: {recall:.4f}
                - **F-Measure**: {f1:.4f}
                """)
            
            with col2:
                fig_cm = plot_confusion_matrix_with_metrics(y_test, dt_pred, "Decision Tree")
                st.pyplot(fig_cm)
            
            # Visualisation de l'arbre (White box representation)
            st.markdown("#### Visualisation de l'Arbre de D√©cision (White Box)")
            st.markdown("*Structure facile √† comprendre et interpr√©ter*")
            
            fig, ax = plt.subplots(figsize=(20, 12))
            plot_tree(dt_model, feature_names=features, class_names=['D√©c√©d√©', 'Survivant'], 
                     filled=True, ax=ax, fontsize=10, max_depth=3)
            ax.set_title("Arbre de D√©cision - Titanic (3 premiers niveaux)")
            st.pyplot(fig)
            
            # Extraction des r√®gles de d√©cision
            st.markdown("#### R√®gles de D√©cision Extraites")
            st.markdown("*L'arbre peut √™tre traduit en ensemble de r√®gles logiques*")
            
            # Simulation de quelques r√®gles principales
            st.markdown("""
            **Exemples de r√®gles extraites:**
            - Si (Sex_encoded = 0) ET (Pclass ‚â§ 2) ALORS Survie = Oui
            - Si (Sex_encoded = 1) ET (Age > 9.5) ALORS Survie = Non
            - Si (Sex_encoded = 0) ET (Fare > 23) ALORS Survie = Oui
            """)
    
    with tab3:
        st.subheader("XGBoost Classifier")
        st.markdown("*Ensemble method - Boosting (Black box representation)*")
        
        # Rappel th√©orique
        st.markdown("""
        **XGBoost (Extreme Gradient Boosting):**
        - **Type**: Black box representation
        - **Principe**: Ensemble de weak learners (arbres)
        - **Avantage**: Tr√®s haute performance
        - **Inconv√©nient**: Difficile √† interpr√©ter
        """)
        
        if st.button("üîÑ Entra√Æner XGBoost", key="xgb_titanic"):
            # Entra√Ænement
            xgb_model = xgb.XGBClassifier(random_state=42, eval_metric='logloss')
            xgb_model.fit(X_train, y_train)
            
            # Pr√©dictions
            xgb_pred = xgb_model.predict(X_test)
            xgb_accuracy = accuracy_score(y_test, xgb_pred)
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.metric("Accuracy", f"{xgb_accuracy:.4f}")
                
                # M√©triques d√©taill√©es
                precision = precision_score(y_test, xgb_pred)
                recall = recall_score(y_test, xgb_pred)
                f1 = f1_score(y_test, xgb_pred)
                
                st.markdown(f"""
                **M√©triques d'√©valuation:**
                - **Precision**: {precision:.4f}
                - **Recall (TPR)**: {recall:.4f}
                - **F-Measure**: {f1:.4f}
                """)
                
                st.text("Rapport de Classification:")
                st.text(classification_report(y_test, xgb_pred))
            
            with col2:
                fig_cm = plot_confusion_matrix_with_metrics(y_test, xgb_pred, "XGBoost")
                st.pyplot(fig_cm)
            
            # Feature importance (seule interpr√©tabilit√© possible)
            st.markdown("#### Feature Importance - XGBoost")
            st.markdown("*Seule forme d'interpr√©tabilit√© pour ce mod√®le black box*")
            
            feature_importance = pd.DataFrame({
                'Feature': features,
                'Importance': xgb_model.feature_importances_
            }).sort_values('Importance', ascending=False)
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.dataframe(feature_importance)
            
            with col2:
                fig, ax = plt.subplots(figsize=(10, 6))
                sns.barplot(data=feature_importance, x='Importance', y='Feature', ax=ax)
                ax.set_title('Importance des Features - XGBoost')
                st.pyplot(fig)
    
    with tab4:
        st.subheader("Comparaison et √âvaluation des Classifiers")
        st.markdown("*Analyse du bias-variance trade-off et complexit√© des mod√®les*")
        
        if st.button("üîÑ Comparer tous les mod√®les", key="compare_titanic"):
            # Pr√©paration des donn√©es pour tous les mod√®les
            X_train_cat = X_train.copy()
            X_test_cat = X_test.copy()
            X_train_cat['Age'] = pd.cut(X_train_cat['Age'], bins=5, labels=False)
            X_test_cat['Age'] = pd.cut(X_test_cat['Age'], bins=5, labels=False)
            X_train_cat['Fare'] = pd.cut(X_train_cat['Fare'], bins=5, labels=False)
            X_test_cat['Fare'] = pd.cut(X_test_cat['Fare'], bins=5, labels=False)
            
            # Entra√Ænement de tous les mod√®les
            models = {
                'Naive Bayes': CategoricalNB(),
                'Decision Tree': DecisionTreeClassifier(random_state=42, max_depth=5),
                'XGBoost': xgb.XGBClassifier(random_state=42, eval_metric='logloss')
            }
            
            results = {}
            detailed_metrics = {}
            
            # Naive Bayes avec donn√©es cat√©gorielles
            models['Naive Bayes'].fit(X_train_cat, y_train)
            nb_pred = models['Naive Bayes'].predict(X_test_cat)
            results['Naive Bayes'] = accuracy_score(y_test, nb_pred)
            detailed_metrics['Naive Bayes'] = {
                'Precision': precision_score(y_test, nb_pred),
                'Recall': recall_score(y_test, nb_pred),
                'F-Measure': f1_score(y_test, nb_pred)
            }
            
            # Decision Tree et XGBoost
            for name in ['Decision Tree', 'XGBoost']:
                models[name].fit(X_train, y_train)
                pred = models[name].predict(X_test)
                results[name] = accuracy_score(y_test, pred)
                detailed_metrics[name] = {
                    'Precision': precision_score(y_test, pred),
                    'Recall': recall_score(y_test, pred),
                    'F-Measure': f1_score(y_test, pred)
                }
            
            # Tableau de comparaison
            comparison_df = pd.DataFrame({
                'Mod√®le': list(results.keys()),
                'Accuracy': list(results.values()),
                'Precision': [detailed_metrics[model]['Precision'] for model in results.keys()],
                'Recall': [detailed_metrics[model]['Recall'] for model in results.keys()],
                'F-Measure': [detailed_metrics[model]['F-Measure'] for model in results.keys()]
            }).round(4)
            
            comparison_df = comparison_df.sort_values('Accuracy', ascending=False)
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.write("**Tableau Comparatif des Performances:**")
                st.dataframe(comparison_df)
                
                # Analyse selon les concepts du cours
                best_model = comparison_df.iloc[0]['Mod√®le']
                best_accuracy = comparison_df.iloc[0]['Accuracy']
                
                st.markdown("###  Analyse selon les Concepts du Cours")
                st.markdown(f"""
                **Meilleur mod√®le**: {best_model} (Accuracy: {best_accuracy:.4f})
                **Bias-Variance Trade-off:**
                - **Naive Bayes**: Bias √©lev√©, Variance faible (hypoth√®se d'ind√©pendance forte)
                - **Decision Tree**: Bias mod√©r√©, Variance mod√©r√©e (contr√¥l√© par max_depth)
                - **XGBoost**: Bias faible, Variance √©lev√©e (ensemble method complexe)
                
                **Complexit√© des mod√®les:**
                - **Naive Bayes**: Tr√®s simple (White box)
                - **Decision Tree**: Simple √† interpr√©ter (White box)
                - **XGBoost**: Tr√®s complexe (Black box)
                
                **Facteurs discriminants identifi√©s:**
                - Sexe (Sex_encoded): Facteur le plus important
                - Classe sociale (Pclass): Impact significatif
                - √Çge: Influence mod√©r√©e
                """)
            
            with col2:
                # Graphique de comparaison multi-m√©triques
                fig, ax = plt.subplots(figsize=(12, 8))
                
                metrics = ['Accuracy', 'Precision', 'Recall', 'F-Measure']
                x = np.arange(len(comparison_df))
                width = 0.2
                
                for i, metric in enumerate(metrics):
                    ax.bar(x + i*width, comparison_df[metric], width, label=metric)
                
                ax.set_xlabel('Mod√®les')
                ax.set_ylabel('Score')
                ax.set_title('Comparaison Multi-M√©triques des Classifiers')
                ax.set_xticks(x + width * 1.5)
                ax.set_xticklabels(comparison_df['Mod√®le'])
                ax.legend()
                ax.grid(True, alpha=0.3)
                st.pyplot(fig)
            
            # ROC Space Analysis (pour classifiers binaires)
            st.markdown("### Analyse ROC Space")
            st.markdown("*√âvaluation selon les concepts de TPR et FPR du cours*")
            
            fig, axes = plt.subplots(1, 3, figsize=(18, 5))
            
            predictions = {
                'Naive Bayes': nb_pred,
                'Decision Tree': models['Decision Tree'].predict(X_test),
                'XGBoost': models['XGBoost'].predict(X_test)
            }
            
            for i, (name, pred) in enumerate(predictions.items()):
                cm = confusion_matrix(y_test, pred)
                
                # Calcul TPR et FPR selon le cours
                TP, TN, FP, FN = cm[1,1], cm[0,0], cm[0,1], cm[1,0]
                TPR = TP / (TP + FN)  # Recall/Sensitivity
                FPR = FP / (FP + TN)  # Fall-out
                
                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[i])
                axes[i].set_title(f'{name}\nTPR: {TPR:.3f}, FPR: {FPR:.3f}')
                axes[i].set_xlabel('Pr√©dictions')
                axes[i].set_ylabel('Valeurs R√©elles')
            
            plt.tight_layout()
            st.pyplot(fig)

###################### PARTIE II: HEART DISEASE DATASET #################
elif section == "Partie II: Heart Disease Dataset":
    st.markdown("#  Partie II: Heart Disease UCI Dataset")
    st.markdown("### *Application avanc√©e des concepts de classification supervis√©e*")
    st.markdown("---")
    
    if heart_data is not None:
        
        # √âtape 1: Analyse du dataset m√©dical
        st.markdown("## √âtape 1: Analyse du Dataset M√©dical")
        st.markdown("*Training set repr√©sentatif pour le diagnostic cardiaque*")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("#### Aper√ßu des donn√©es cliniques")
            st.dataframe(heart_data.head())
            
            st.markdown("#### Informations sur les attributs m√©dicaux")
            info_heart = pd.DataFrame({
                "Attributs": heart_data.columns,
                "Type": heart_data.dtypes.astype(str),
                "Non-null": heart_data.notnull().sum(),
                "Valeurs manquantes": heart_data.isnull().sum(),
                "% Manquant": round(heart_data.isnull().mean()*100, 2)
            })
            st.dataframe(info_heart)
        
        with col2:
            st.markdown("#### Statistiques descriptives")
            st.dataframe(heart_data.describe())
            
            # V√©rification de la qualit√© des donn√©es
            missing_values = heart_data.isnull().sum().sum()
            if missing_values == 0:
                st.success("Dataset de haute qualit√©: Aucune valeur manquante")
            else:
                st.warning(f" {missing_values} valeurs manquantes √† traiter")
            
            # Distribution de la variable cible
            st.markdown("#### Distribution du Diagnostic")
            target_counts = heart_data['target'].value_counts()
            fig, ax = plt.subplots(figsize=(8, 6))
            ax.pie(target_counts.values, labels=['Pas de maladie (0)', 'Maladie (1)'], 
                   autopct='%1.1f%%', colors=['lightgreen', 'lightcoral'])
            ax.set_title('Distribution du Diagnostic Cardiaque')
            st.pyplot(fig)
        
        # √âtape 2: Feature Engineering m√©dical
        st.markdown("## √âtape 2: Feature Engineering M√©dical")
        st.markdown("*Cr√©ation d'attributs cliniquement pertinents*")
        
        heart_processed = heart_data.copy()
        
        # Cr√©ation de features m√©dicales selon les standards cliniques
        heart_processed['Age_Group'] = pd.cut(
            heart_processed['age'], 
            bins=[0, 30, 40, 50, 60, 100], 
            labels=['<30', '30-40', '40-50', '50-60', '60+']
        )
        
        # Classification du cholest√©rol selon les standards m√©dicaux
        heart_processed['Cholesterol_Range'] = pd.cut(
            heart_processed['chol'],
            bins=[0, 200, 240, 300, 600],
            labels=['Normal (<200)', 'Limite (200-240)', '√âlev√© (240-300)', 'Tr√®s √©lev√© (>300)']
        )
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("#### Groupes d'√Çge (Feature Engineering)")
            age_group_analysis = heart_processed.groupby('Age_Group').agg({
                'target': ['count', 'mean']
            }).round(3)
            age_group_analysis.columns = ['Effectif', 'P(Maladie|√Çge)']
            st.dataframe(age_group_analysis)
            
            fig, ax = plt.subplots()
            age_group_analysis['P(Maladie|√Çge)'].plot(kind='bar', ax=ax, color='skyblue')
            ax.set_title('P(Maladie | Groupe d\'√Çge)')
            ax.set_ylabel('Probabilit√© de Maladie')
            plt.xticks(rotation=45)
            st.pyplot(fig)
        
        with col2:
            st.markdown("#### Plages de Cholest√©rol (Standards M√©dicaux)")
            chol_analysis = heart_processed.groupby('Cholesterol_Range').agg({
                'target': ['count', 'mean']
            }).round(3)
            chol_analysis.columns = ['Effectif', 'P(Maladie|Cholest√©rol)']
            st.dataframe(chol_analysis)
            
            fig, ax = plt.subplots()
            chol_analysis['P(Maladie|Cholest√©rol)'].plot(kind='bar', ax=ax, color='lightcoral')
            ax.set_title('P(Maladie | Cholest√©rol)')
            ax.set_ylabel('Probabilit√© de Maladie')
            plt.xticks(rotation=45)
            st.pyplot(fig)
        
        # Analyse des corr√©lations (important pour Naive Bayes)
        st.markdown("## Analyse des Corr√©lations")
        st.markdown("*V√©rification de l'hypoth√®se d'ind√©pendance pour Naive Bayes*")
        
        with st.expander(" Matrice de corr√©lation et visualisations"):
            fig, axes = plt.subplots(2, 2, figsize=(16, 12))
            
            # Matrice de corr√©lation
            numeric_cols = heart_processed.select_dtypes(include=[np.number]).columns
            correlation_matrix = heart_processed[numeric_cols].corr()
            
            sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, ax=axes[0,0])
            axes[0,0].set_title('Matrice de Corr√©lation - Attributs M√©dicaux')
            
            # Distribution par sexe
            heart_by_sex = heart_processed.groupby('sex')['target'].mean()
            axes[0,1].bar(['Femme (0)', 'Homme (1)'], heart_by_sex.values, color=['pink', 'lightblue'])
            axes[0,1].set_title('P(Maladie | Sexe)')
            axes[0,1].set_ylabel('Probabilit√© de Maladie')
            
            # Distribution de l'√¢ge
            axes[1,0].hist(heart_processed['age'], bins=20, alpha=0.7, color='skyblue', edgecolor='black')
            axes[1,0].set_title('Distribution des √Çges')
            axes[1,0].set_xlabel('√Çge')
            axes[1,0].set_ylabel('Fr√©quence')
            
            # Distribution du cholest√©rol
            axes[1,1].hist(heart_processed['chol'], bins=20, alpha=0.7, color='lightcoral', edgecolor='black')
            axes[1,1].set_title('Distribution du Cholest√©rol')
            axes[1,1].set_xlabel('Cholest√©rol (mg/dl)')
            axes[1,1].set_ylabel('Fr√©quence')
            
            plt.tight_layout()
            st.pyplot(fig)
        
        # Pr√©paration pour les mod√®les
        st.markdown("## √âtapes 3-5: Mod√®les de Classification M√©dicale")
        
        # Encodage des nouvelles features
        le_age_group = LabelEncoder()
        le_chol_range = LabelEncoder()
        
        heart_processed['Age_Group_encoded'] = le_age_group.fit_transform(heart_processed['Age_Group'])
        heart_processed['Cholesterol_Range_encoded'] = le_chol_range.fit_transform(heart_processed['Cholesterol_Range'])
        
        # S√©lection des features m√©dicales
        feature_cols = [col for col in heart_processed.columns if col not in ['target', 'Age_Group', 'Cholesterol_Range']]
        X_heart = heart_processed[feature_cols]
        y_heart = heart_processed['target']
        
        # Division selon le protocole m√©dical
        X_train_heart, X_test_heart, y_train_heart, y_test_heart = train_test_split(
            X_heart, y_heart, test_size=0.2, random_state=42, stratify=y_heart
        )
        
        st.success(f"**Training set m√©dical**: {X_train_heart.shape[0]} patients | **Test set**: {X_test_heart.shape[0]} patients")
        st.info(f"üè• **Attributs cliniques**: {len(feature_cols)} features (incluant les nouvelles)")
        
        # Tabs pour les mod√®les m√©dicaux
        tab1, tab2, tab3, tab4 = st.tabs([
            " Naive Bayes (Gaussian)", 
            " Decision Tree M√©dical", 
            " XGBoost Diagnostic", 
            " √âvaluation Clinique"
        ])
        
        with tab1:
            st.subheader("Naive Bayes Classifier (Gaussian)")
            st.markdown("*Adapt√© aux donn√©es m√©dicales continues*")
            
            # Rappel th√©orique pour donn√©es m√©dicales
            st.markdown("""
            **Gaussian Naive Bayes pour donn√©es m√©dicales:**
            - Assume que chaque attribut suit une distribution normale
            - Adapt√© aux mesures physiologiques (√¢ge, cholest√©rol, pression, etc.)
            - P(attribut|classe) ~ N(Œº, œÉ¬≤)
            """)
            
            if st.button("üîÑ Entra√Æner Naive Bayes M√©dical", key="nb_heart"):
                # Entra√Ænement avec GaussianNB pour donn√©es continues
                nb_model = GaussianNB()
                nb_model.fit(X_train_heart, y_train_heart)
                
                # Pr√©dictions
                nb_pred = nb_model.predict(X_test_heart)
                nb_accuracy = accuracy_score(y_test_heart, nb_pred)
                
                col1, col2 = st.columns(2)
                
                with col1:
                    st.metric("Accuracy Diagnostique", f"{nb_accuracy:.4f}")
                    
                    # M√©triques critiques en m√©decine
                    precision = precision_score(y_test_heart, nb_pred)
                    recall = recall_score(y_test_heart, nb_pred)
                    f1 = f1_score(y_test_heart, nb_pred)
                    
                    st.markdown(f"""
                    **M√©triques Cliniques:**
                    - **Precision**: {precision:.4f} (Fiabilit√© du diagnostic positif)
                    - **Recall/Sensibilit√©**: {recall:.4f} (D√©tection des vrais malades)
                    - **F-Measure**: {f1:.4f} (√âquilibre global)
                    
                    **Interpr√©tation m√©dicale:**
                    - Recall √©lev√© = Peu de faux n√©gatifs (important en m√©decine)
                    - Precision √©lev√©e = Peu de faux positifs (√©vite sur-traitement)
                    """)
                
                with col2:
                    fig_cm = plot_confusion_matrix_with_metrics(y_test_heart, nb_pred, "Naive Bayes M√©dical")
                    st.pyplot(fig_cm)
                
                # Analyse des probabilit√©s diagnostiques
                st.markdown("#### Analyse des Probabilit√©s Diagnostiques")
                nb_proba = nb_model.predict_proba(X_test_heart)
                
                fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
                
                # Distribution des probabilit√©s
                ax1.hist(nb_proba[:, 1], bins=20, alpha=0.7, color='lightcoral', edgecolor='black')
                ax1.set_title('Distribution P(Maladie|Sympt√¥mes)')
                ax1.set_xlabel('Probabilit√© de Maladie Cardiaque')
                ax1.set_ylabel('Nombre de Patients')
                ax1.axvline(x=0.5, color='red', linestyle='--', label='Seuil de d√©cision')
                ax1.legend()
                
                # Analyse par seuil de probabilit√©
                thresholds = np.arange(0.1, 1.0, 0.1)
                sensitivities = []
                specificities = []
                
                for threshold in thresholds:
                    pred_thresh = (nb_proba[:, 1] >= threshold).astype(int)
                    cm = confusion_matrix(y_test_heart, pred_thresh)
                    if cm.shape == (2, 2):
                        TP, TN, FP, FN = cm[1,1], cm[0,0], cm[0,1], cm[1,0]
                        sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0
                        specificity = TN / (TN + FP) if (TN + FP) > 0 else 0
                        sensitivities.append(sensitivity)
                        specificities.append(specificity)
                
                ax2.plot(thresholds, sensitivities, 'b-', label='Sensibilit√© (Recall)', marker='o')
                ax2.plot(thresholds, specificities, 'r-', label='Sp√©cificit√©', marker='s')
                ax2.set_title('Sensibilit√© vs Sp√©cificit√© par Seuil')
                ax2.set_xlabel('Seuil de Probabilit√©')
                ax2.set_ylabel('Score')
                ax2.legend()
                ax2.grid(True, alpha=0.3)
                
                st.pyplot(fig)
        
        with tab2:
            st.subheader("Decision Tree pour Diagnostic M√©dical")
            st.markdown("*R√®gles de d√©cision clinique interpr√©tables*")
            
            # Rappel de l'importance en m√©decine
            st.markdown("""
            **Avantages en contexte m√©dical:**
            - **Interpr√©tabilit√©**: Les m√©decins peuvent suivre le raisonnement
            - **R√®gles cliniques**: Extraction de protocoles de diagnostic
            - **Transparence**: Confiance dans les d√©cisions automatis√©es
            """)
            
            if st.button("üîÑ Entra√Æner Decision Tree M√©dical", key="dt_heart"):
                # Entra√Ænement avec profondeur contr√¥l√©e pour √©viter l'overfitting
                dt_model = DecisionTreeClassifier(random_state=42, max_depth=5, min_samples_split=10)
                dt_model.fit(X_train_heart, y_train_heart)
                
                # Pr√©dictions
                dt_pred = dt_model.predict(X_test_heart)
                dt_accuracy = accuracy_score(y_test_heart, dt_pred)
                
                col1, col2 = st.columns(2)
                
                with col1:
                    st.metric("Accuracy Diagnostique", f"{dt_accuracy:.4f}")
                    
                    # Feature importance m√©dicale
                    feature_importance = pd.DataFrame({
                        'Attribut M√©dical': feature_cols,
                        'Importance Clinique': dt_model.feature_importances_
                    }).sort_values('Importance Clinique', ascending=False)
                    
                    st.write("**Importance des Attributs M√©dicaux:**")
                    st.dataframe(feature_importance.head(10))
                    
                    # M√©triques cliniques
                    precision = precision_score(y_test_heart, dt_pred)
                    recall = recall_score(y_test_heart, dt_pred)
                    f1 = f1_score(y_test_heart, dt_pred)
                    
                    st.markdown(f"""
                    **Performance Clinique:**
                    - **Precision**: {precision:.4f}
                    - **Sensibilit√©**: {recall:.4f}
                    - **F-Measure**: {f1:.4f}
                    """)
                
                with col2:
                    fig_cm = plot_confusion_matrix_with_metrics(y_test_heart, dt_pred, "Decision Tree M√©dical")
                    st.pyplot(fig_cm)
                
                # Visualisation de l'arbre de d√©cision m√©dical
                st.markdown("#### Arbre de D√©cision Clinique")
                st.markdown("*Protocole de diagnostic automatis√©*")
                
                fig, ax = plt.subplots(figsize=(20, 12))
                plot_tree(dt_model, feature_names=feature_cols, 
                         class_names=['Pas de maladie', 'Maladie cardiaque'], 
                         filled=True, ax=ax, fontsize=8, max_depth=3)
                ax.set_title("Arbre de D√©cision - Diagnostic Cardiaque (3 premiers niveaux)")
                st.pyplot(fig)
                
                # Extraction de r√®gles cliniques
                st.markdown("#### R√®gles de Diagnostic Extraites")
                st.markdown("*Protocoles cliniques automatiquement g√©n√©r√©s*")
                
                # Simulation de r√®gles bas√©es sur les features importantes
                top_features = feature_importance.head(3)['Attribut M√©dical'].tolist()
                
                st.markdown(f"""
                **Exemples de r√®gles cliniques extraites:**
                
                Bas√©es sur les attributs les plus discriminants: {', '.join(top_features)}
                
                - Si (cp ‚â§ 0.5) ET (thalach > 150) ALORS Risque Faible
                - Si (cp > 2.5) ET (oldpeak > 1.0) ALORS Risque √âlev√©
                - Si (ca > 0) ET (thal ‚â§ 2) ALORS Examen Compl√©mentaire Requis
                
                *Ces r√®gles peuvent √™tre valid√©es par des cardiologues*
                """)
                
                # Graphique d'importance des features
                fig, ax = plt.subplots(figsize=(12, 8))
                sns.barplot(data=feature_importance.head(10), 
                           x='Importance Clinique', y='Attribut M√©dical', ax=ax)
                ax.set_title('Importance des Attributs pour le Diagnostic')
                st.pyplot(fig)
        
        with tab3:
            st.subheader("XGBoost pour Diagnostic Avanc√©")
            st.markdown("*Mod√®le haute performance pour aide au diagnostic*")
            
            # Contexte m√©dical
            st.markdown("""
            **XGBoost en contexte m√©dical:**
            - **Performance maximale**: D√©tection optimale des patterns complexes
            - **Ensemble learning**: Combine multiple arbres de d√©cision
            - **Limitation**: Mod√®le "bo√Æte noire" difficile √† expliquer aux m√©decins
            """)
            
            if st.button("üîÑ Entra√Æner XGBoost M√©dical", key="xgb_heart"):
                # Entra√Ænement avec validation
                xgb_model = xgb.XGBClassifier(random_state=42, eval_metric='logloss', n_estimators=100)
                xgb_model.fit(X_train_heart, y_train_heart)
                
                # Pr√©dictions
                xgb_pred = xgb_model.predict(X_test_heart)
                xgb_accuracy = accuracy_score(y_test_heart, xgb_pred)
                
                col1, col2 = st.columns(2)
                
                with col1:
                    st.metric("Accuracy Diagnostique", f"{xgb_accuracy:.4f}")
                    
                    # M√©triques critiques pour le diagnostic m√©dical
                    precision = precision_score(y_test_heart, xgb_pred)
                    recall = recall_score(y_test_heart, xgb_pred)
                    f1 = f1_score(y_test_heart, xgb_pred)
                    
                    st.markdown(f"""
                    **Performance Diagnostique:**
                    - **Precision**: {precision:.4f}
                    - **Sensibilit√© (Recall)**: {recall:.4f}
                    - **F-Measure**: {f1:.4f}
                    
                    **Interpr√©tation clinique:**
                    - Sensibilit√© √©lev√©e = D√©tection efficace des malades
                    - Precision √©lev√©e = Peu de faux diagnostics positifs
                    """)
                    
                    st.text("Rapport de Classification D√©taill√©:")
                    st.text(classification_report(y_test_heart, xgb_pred, 
                                                target_names=['Sain', 'Maladie Cardiaque']))
                
                with col2:
                    fig_cm = plot_confusion_matrix_with_metrics(y_test_heart, xgb_pred, "XGBoost M√©dical")
                    st.pyplot(fig_cm)
                
                # Feature importance (seule interpr√©tabilit√© possible)
                st.markdown("#### Importance des Attributs M√©dicaux")
                st.markdown("*Facteurs de risque identifi√©s par l'algorithme*")
                
                feature_importance = pd.DataFrame({
                    'Attribut M√©dical': feature_cols,
                    'Score d\'Importance': xgb_model.feature_importances_
                }).sort_values('Score d\'Importance', ascending=False)
                
                col1, col2 = st.columns(2)
                
                with col1:
                    st.dataframe(feature_importance.head(10))
                    
                    # Interpr√©tation m√©dicale des top features
                    top_3_features = feature_importance.head(3)['Attribut M√©dical'].tolist()
                    st.markdown(f"""
                    **Top 3 Facteurs de Risque:**
                    1. **{top_3_features[0]}**: Facteur principal
                    2. **{top_3_features[1]}**: Facteur secondaire  
                    3. **{top_3_features[2]}**: Facteur tertiaire
                    
                    *Ces facteurs n√©cessitent une validation clinique*
                    """)
                
                with col2:
                    fig, ax = plt.subplots(figsize=(10, 8))
                    sns.barplot(data=feature_importance.head(10), 
                               x='Score d\'Importance', y='Attribut M√©dical', ax=ax)
                    ax.set_title('Facteurs de Risque - XGBoost')
                    st.pyplot(fig)
                
                # Courbes d'apprentissage pour validation
                st.markdown("#### Validation du Mod√®le")
                st.markdown("*Analyse de la convergence et du sur-apprentissage*")
                
                # Entra√Ænement avec suivi des m√©triques
                eval_set = [(X_train_heart, y_train_heart), (X_test_heart, y_test_heart)]
                xgb_model_eval = xgb.XGBClassifier(random_state=42, eval_metric='logloss', n_estimators=100)
                xgb_model_eval.fit(X_train_heart, y_train_heart, eval_set=eval_set, verbose=False)
                
                results = xgb_model_eval.evals_result()
                
                fig, ax = plt.subplots(figsize=(12, 6))
                ax.plot(results['validation_0']['logloss'], label='Training Loss', color='blue')
                ax.plot(results['validation_1']['logloss'], label='Validation Loss', color='red')
                ax.set_title('Courbes d\'Apprentissage - Validation M√©dicale')
                ax.set_xlabel('It√©rations (Boosting Rounds)')
                ax.set_ylabel('Log Loss')
                ax.legend()
                ax.grid(True, alpha=0.3)
                
                # Annotation des zones d'int√©r√™t
                ax.axhline(y=min(results['validation_1']['logloss']), 
                          color='green', linestyle='--', alpha=0.7, 
                          label='Optimum Validation')
                
                st.pyplot(fig)
        
        with tab4:
            st.subheader("√âvaluation Clinique Comparative")
            st.markdown("*Analyse selon les standards m√©dicaux et concepts du cours*")
            
            if st.button("üîÑ √âvaluation Clinique Compl√®te", key="compare_heart"):
                # Entra√Ænement de tous les mod√®les
                models = {
                    'Naive Bayes (Gaussian)': GaussianNB(),
                    'Decision Tree': DecisionTreeClassifier(random_state=42, max_depth=5, min_samples_split=10),
                    'XGBoost': xgb.XGBClassifier(random_state=42, eval_metric='logloss')
                }
                
                results = {}
                predictions = {}
                probabilities = {}
                
                for name, model in models.items():
                    model.fit(X_train_heart, y_train_heart)
                    pred = model.predict(X_test_heart)
                    
                    # Probabilit√©s pour analyse ROC
                    if hasattr(model, 'predict_proba'):
                        proba = model.predict_proba(X_test_heart)[:, 1]
                    else:
                        proba = model.decision_function(X_test_heart)
                    
                    results[name] = accuracy_score(y_test_heart, pred)
                    predictions[name] = pred
                    probabilities[name] = proba
                
                # M√©triques d√©taill√©es selon les standards m√©dicaux
                detailed_results = []
                for name, pred in predictions.items():
                    cm = confusion_matrix(y_test_heart, pred)
                    TP, TN, FP, FN = cm[1,1], cm[0,0], cm[0,1], cm[1,0]
                    
                    accuracy = (TP + TN) / (TP + TN + FP + FN)
                    precision = TP / (TP + FP) if (TP + FP) > 0 else 0
                    recall = TP / (TP + FN) if (TP + FN) > 0 else 0  # Sensibilit√©
                    specificity = TN / (TN + FP) if (TN + FP) > 0 else 0
                    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
                    
                    detailed_results.append({
                        'Mod√®le': name,
                        'Accuracy': accuracy,
                        'Precision': precision,
                        'Sensibilit√© (Recall)': recall,
                        'Sp√©cificit√©': specificity,
                        'F-Measure': f1
                    })
                
                detailed_df = pd.DataFrame(detailed_results)
                detailed_df = detailed_df.sort_values('F-Measure', ascending=False)
                
                col1, col2 = st.columns(2)
                
                with col1:
                    st.write("**√âvaluation Clinique Comparative:**")
                    st.dataframe(detailed_df.round(4))
                    
                    # Analyse selon les concepts du cours
                    best_model = detailed_df.iloc[0]['Mod√®le']
                    best_f1 = detailed_df.iloc[0]['F-Measure']
                    
                    st.markdown("### üè• Analyse Clinique selon les Concepts du Cours")
                    st.markdown(f"""
                    **Meilleur mod√®le clinique**: {best_model} (F-Measure: {best_f1:.4f})
                    
                    **Bias-Variance Trade-off en contexte m√©dical:**
                    - **Naive Bayes**: Bias √©lev√© (hypoth√®se d'ind√©pendance), Variance faible
                      - Avantage: Rapide, simple √† expliquer aux m√©decins
                      - Inconv√©nient: Peut manquer des interactions complexes
                    
                    - **Decision Tree**: Bias mod√©r√©, Variance contr√¥l√©e
                      - Avantage: Tr√®s interpr√©table, r√®gles cliniques claires
                      - Inconv√©nient: Peut √™tre instable avec petites variations
                    
                    - **XGBoost**: Bias faible, Variance √©lev√©e
                      - Avantage: Performance maximale, d√©tection de patterns complexes
                      - Inconv√©nient: "Bo√Æte noire", difficile √† expliquer
                    
                    **Recommandations cliniques:**
                    - **Pour screening initial**: Naive Bayes (rapide, explicable)
                    - **Pour protocoles cliniques**: Decision Tree (r√®gles claires)
                    - **Pour diagnostic expert**: XGBoost (performance maximale)
                    """)
                
                with col2:
                    # Graphique radar des performances
                    fig, ax = plt.subplots(figsize=(10, 8), subplot_kw=dict(projection='polar'))
                    
                    metrics = ['Accuracy', 'Precision', 'Sensibilit√© (Recall)', 'Sp√©cificit√©', 'F-Measure']
                    angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()
                    angles += angles[:1]  # Fermer le cercle
                    
                    colors = ['blue', 'green', 'red']
                    
                    for i, (_, row) in enumerate(detailed_df.iterrows()):
                        values = [row[metric] for metric in metrics]
                        values += values[:1]  # Fermer le cercle
                        
                        ax.plot(angles, values, 'o-', linewidth=2, 
                               label=row['Mod√®le'], color=colors[i])
                        ax.fill(angles, values, alpha=0.25, color=colors[i])
                    
                    ax.set_xticks(angles[:-1])
                    ax.set_xticklabels(metrics)
                    ax.set_ylim(0, 1)
                    ax.set_title('Performance Radar - √âvaluation Clinique')
                    ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))
                    ax.grid(True)
                    
                    st.pyplot(fig)
                
                # Analyse des erreurs critiques en m√©decine
                st.markdown("###  Analyse des Erreurs Critiques")
                st.markdown("*Faux n√©gatifs vs Faux positifs en contexte m√©dical*")
                
                fig, axes = plt.subplots(1, 3, figsize=(18, 6))
                
                for i, (name, pred) in enumerate(predictions.items()):
                    cm = confusion_matrix(y_test_heart, pred)
                    TP, TN, FP, FN = cm[1,1], cm[0,0], cm[0,1], cm[1,0]
                    
                    # Calcul des co√ªts m√©dicaux
                    cost_fn = FN * 10  # Co√ªt √©lev√© des faux n√©gatifs (maladie non d√©tect√©e)
                    cost_fp = FP * 1   # Co√ªt mod√©r√© des faux positifs (examens suppl√©mentaires)
                    total_cost = cost_fn + cost_fp
                    
                    sns.heatmap(cm, annot=True, fmt='d', cmap='Reds', ax=axes[i])
                    axes[i].set_title(f'{name}\nCo√ªt M√©dical: {total_cost}\nFN√ó10 + FP√ó1')
                    axes[i].set_xlabel('Pr√©dictions')
                    axes[i].set_ylabel('R√©alit√© Clinique')
                
                plt.tight_layout()
                st.pyplot(fig)
                
                # Analyse des cas d'erreur
                st.markdown("###  Analyse des Cas d'Erreur")
                
                # Prendre le meilleur mod√®le pour l'analyse
                best_model_name = detailed_df.iloc[0]['Mod√®le']
                best_model_obj = models[best_model_name]
                best_pred = predictions[best_model_name]
                
                # Identifier les erreurs
                errors_df = X_test_heart.copy()
                errors_df['Diagnostic_Reel'] = y_test_heart
                errors_df['Diagnostic_Predit'] = best_pred
                errors_df['Correct'] = (y_test_heart == best_pred)
                
                false_positives = errors_df[(errors_df['Diagnostic_Reel'] == 0) & (errors_df['Diagnostic_Predit'] == 1)]
                false_negatives = errors_df[(errors_df['Diagnostic_Reel'] == 1) & (errors_df['Diagnostic_Predit'] == 0)]
                
                col1, col2 = st.columns(2)
                
                with col1:
                    st.markdown(f"####  Faux Positifs: {len(false_positives)} cas")
                    st.markdown("*(Pr√©dits malades mais sains - Sur-diagnostic)*")
                    
                    if len(false_positives) > 0:
                        st.write("**Profil moyen des faux positifs:**")
                        fp_profile = false_positives[['age', 'chol', 'thalach', 'oldpeak']].mean()
                        
                        profile_df = pd.DataFrame({
                            'Attribut': ['√Çge', 'Cholest√©rol', 'Freq. Cardiaque Max', 'D√©pression ST'],
                            'Valeur Moyenne': fp_profile.values
                        }).round(1)
                        st.dataframe(profile_df)
                        
                        st.markdown("""
                        **Impact clinique:**
                        - Examens compl√©mentaires inutiles
                        - Stress psychologique du patient
                        - Co√ªts de sant√© suppl√©mentaires
                        """)
                
                with col2:
                    st.markdown(f"####  Faux N√©gatifs: {len(false_negatives)} cas")
                    st.markdown("*(Pr√©dits sains mais malades - Sous-diagnostic)*")
                    
                    if len(false_negatives) > 0:
                        st.write("**Profil moyen des faux n√©gatifs:**")
                        fn_profile = false_negatives[['age', 'chol', 'thalach', 'oldpeak']].mean()
                        
                        profile_df = pd.DataFrame({
                            'Attribut': ['√Çge', 'Cholest√©rol', 'Freq. Cardiaque Max', 'D√©pression ST'],
                            'Valeur Moyenne': fn_profile.values
                        }).round(1)
                        st.dataframe(profile_df)
                        
                        st.markdown("""
                        **Impact clinique CRITIQUE:**
                        - Maladie non d√©tect√©e
                        - Absence de traitement pr√©ventif
                        - Risque vital pour le patient
                        """)
                
                # Recommandations cliniques finales
                st.markdown("### Recommandations Cliniques Finales")
                
                col1, col2 = st.columns(2)
                
                with col1:
                    st.markdown("""
                    **Strat√©gie de D√©ploiement Recommand√©e:**
                    
                    1. **Screening de masse**: Naive Bayes
                       - Rapide et explicable
                       - Bon pour identifier les cas √©vidents
                    
                    2. **Diagnostic approfondi**: Decision Tree
                       - R√®gles cliniques claires
                       - Validation possible par cardiologues
                    
                    3. **Cas complexes**: XGBoost
                       - Performance maximale
                       - Aide √† la d√©cision pour cas difficiles
                    """)
                
                with col2:
                    st.markdown("""
                    **M√©triques Prioritaires par Contexte:**
                    
                    - **Urgences**: Maximiser la Sensibilit√© (Recall)
                      - Ne pas manquer de vrais malades
                    
                    - **Consultations**: √âquilibrer Precision/Recall
                      - F-Measure comme m√©trique principale
                    
                    - **Screening**: Optimiser selon les co√ªts
                      - Consid√©rer le ratio FN√ó10 + FP√ó1
                    """)

# Sidebar - Informations compl√©mentaires
st.sidebar.markdown("---")
st.sidebar.markdown("###  Concepts Th√©oriques Appliqu√©s")

if st.sidebar.button(" Bias-Variance Trade-off"):
    st.sidebar.markdown("""
    **Bias-Variance selon le cours:**
    
    - **Bias**: Erreur due aux hypoth√®ses 
      simplificatrices (underfitting)
    - **Variance**: Sensibilit√© aux variations 
      des donn√©es (overfitting)
    
    **√âquilibre optimal:**
    - Mod√®les simples: Bias‚Üë, Variance‚Üì
    - Mod√®les complexes: Bias‚Üì, Variance‚Üë
    """)

if st.sidebar.button(" M√©triques d'√âvaluation"):
    st.sidebar.markdown("""
    **Formules selon le cours:**
    
    - **Accuracy** = (TP + TN) / Total
    - **Precision** = TP / (TP + FP)
    - **Recall (TPR)** = TP / (TP + FN)
    - **Specificity** = TN / (TN + FP)
    - **F-Measure** = 2√ó(P√óR)/(P+R)
    
    **ROC Space:**
    - Axe X: FPR = FP/(FP+TN)
    - Axe Y: TPR = TP/(TP+FN)
    """)

if st.sidebar.button("üî¨ Th√©or√®me de Bayes"):
    st.sidebar.markdown("""
    **Formule fondamentale:**
    
    P(classe|x) = P(x|classe) √ó P(classe) / P(x)
    
    **Hypoth√®se Naive:**
    P(x|classe) = ‚àè P(xi|classe)
    
    **Applications:**
    - Classification probabiliste
    - Diagnostic m√©dical
    - Analyse de risque
    """)

# Footer acad√©mique
st.markdown("---")
st.markdown("""
<div style='text-align: center; color: gray; padding: 20px;'>
    <p><strong>Machine Learning I - TP3 Classification Supervis√©e</strong></p>
    <p><em>Concepts appliqu√©s: Distance-based approaches, Decision trees, Naive Bayes Classifiers</em></p>
    <p>Issam Falih - Department of Computer Science | D√©velopp√© avec Streamlit</p>
    <p>Datasets: Titanic (891 √©chantillons) | Heart Disease UCI (303 patients)</p>
</div>
""", unsafe_allow_html=True)
